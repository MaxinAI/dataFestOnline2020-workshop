{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/cnn/maxinai.png\" height=\"1000\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Detecting Face Mask in Real Time\n",
    "\n",
    "---\n",
    "\n",
    "> * Summary of Convolutional Neural Networks\n",
    "> * Data Collection and Pre-processing\n",
    "> * Model Architecture\n",
    "> * Model Building and Training\n",
    "> * Performance Evaluation\n",
    "> * Face Mask Detection in Real-Time Video Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary of Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computer Vison Tasks\n",
    "\n",
    "\n",
    "<img src=\"images/cnn/vision_tasks.png\" height=\"1000\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Kernel of Convolution\n",
    "\n",
    "---\n",
    "\n",
    "- One of the most important components of a convolution operation is the kernel (a.k.a filter). \n",
    "It is a small matrix and used for blurring, sharpening, embossing, edge detection, and more.\n",
    "\n",
    "[Some well known kernels](https://bit.ly/36iPv2I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1-D Convolution\n",
    "\n",
    "---\n",
    "\n",
    "1D Convolutions are actually just a simplified version of the 2D Convolution.\n",
    "\n",
    "\n",
    "<img src=\"images/cnn/1_D_convolution.png\" height=\"800\" width=\"800\">\n",
    "\n",
    "<center>$(1 \\times 2) + (3 \\times 0) + (3 \\times 1) = 5$</center>\n",
    "\n",
    "[source](https://medium.com/apache-mxnet/1d-3d-convolutions-explained-with-ms-excel-5f88c0f35941)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Padding and Stride\n",
    "\n",
    "---\n",
    "\n",
    "- In order to keep the convolution result size the same size as the input, we pad the sequence with zeros to the left and right. \n",
    "\n",
    "\n",
    "- Stride is a step, number indicating how much of a jump we want to make between evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Padding and Stride\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Padding = 1; Stride = 2\n",
    "\n",
    "<img src=\"images/cnn/1_D_padding_stride.png\" height=\"800\" width=\"800\">\n",
    "\n",
    "<center>$(0 \\times 2) + (1 \\times 0) + (3 \\times 1) = 3$</center>\n",
    "\n",
    "<center>$(3 \\times 2) + (3 \\times 0) + (0 \\times 1) = 6$</center>\n",
    "\n",
    "[source](https://medium.com/apache-mxnet/1d-3d-convolutions-explained-with-ms-excel-5f88c0f35941)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> * Rarely used in image processing\n",
    "> * Used as alternative of RNNs in NLP\n",
    "> * Used in time series modeling\n",
    "\n",
    "The last two fields share one common spatial dimension (time). This type of data is a sequential data and that's why 1-D convolution is suitable!\n",
    "\n",
    "Images have two spatial dimensions, heights and width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2-D Convolution\n",
    "\n",
    "---\n",
    "\n",
    "2D convolutions are used as image filters. With 2D Convolutions we slide the kernel in two directions: left/right and up/down.\n",
    "\n",
    "<img src=\"images/cnn/Conv2dUsage1.png\" height=\"800\" width=\"800\">\n",
    "\n",
    "[source](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolution.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2-D Convolution - No Padding, No Stride\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"images/cnn/2-D_convolution.png\" height=\"800\" width=\"800\">\n",
    "\n",
    "<center>$(1 \\times 1)+(3 \\times 2)+(2 \\times 3)+(1 \\times 0)+(3 \\times 1)+(3 \\times 0)+(2 \\times 2)+(1 \\times 1)+(1 \\times 2) = 23$</center>\n",
    "\n",
    "\n",
    "[source](https://medium.com/apache-mxnet/convolutions-explained-with-ms-excel-465d6649831c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2-D Convolution - Padding, No Stride\n",
    "\n",
    "---\n",
    "\n",
    "With padding we can maintain the spatial dimensions through the convolution operation. Zeros are placed all the way around the original matrix. We need to pad by one element in every direction to maintain the dimensions, and we see the output is now $(4 \\times 4)$\n",
    "\n",
    "<img src=\"images/cnn/2-D_convolution_padding.png\" height=\"800\" width=\"800\">\n",
    "\n",
    "\n",
    "[source](https://medium.com/apache-mxnet/convolutions-explained-with-ms-excel-465d6649831c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2-D Convolution - No Padding, Stride\n",
    "\n",
    "---\n",
    "\n",
    "Stride is an effective method for reducing the spatial dimensions, instead of using pooling operations. Stride indicates how much to jump. In our example jump size is two.\n",
    "\n",
    "<img src=\"images/cnn/2-D_convolution_stride.png\" height=\"800\" width=\"800\">\n",
    "\n",
    "\n",
    "[source](https://medium.com/apache-mxnet/convolutions-explained-with-ms-excel-465d6649831c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2-D Convolution - Max Pooling\n",
    "\n",
    "---\n",
    "\n",
    "We can use pooling instead of stride with step size more than one. This is an example of max pooling.\n",
    "\n",
    "<img src=\"images/cnn/pooling_1.png\" height=\"800\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Greyscale Image Convolution\n",
    "---\n",
    "\n",
    "<img src=\"images/cnn/moving_kernel_1_d.gif\" height=\"800\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### RGB Image Convolution\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"images/cnn/moving_kernel_rgb.gif\" height=\"400\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Complex feature extraction\n",
    "\n",
    "---\n",
    "\n",
    "Stacked layer of convolution extract more complex features from layer to layer up to dense layers which then classify input image\n",
    "\n",
    "<img src=\"images/cnn/face_features.png\" height=\"900\" width=\"1800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Whole model\n",
    "---\n",
    "##### Note that:\n",
    "- Kernel has the same number of channels as input data;  \n",
    "- Each convolution layer has several Kernels (64, 128, ect);\n",
    "- Output of convolutional layer has as many number of channels as many kernels it applied to input data;\n",
    "- Output of convolutional layer pass through activation function (ReLu, Tanh, ect) and then to the next convolutional layer (or pooling layer first and then to the conv. layer);  \n",
    "- The process repeats before dense layers are reached. After that dense layers do the classification task on extracted features;\n",
    "<img src=\"images/cnn/stacked_convs.jpg\" height=\"700\" width=\"1800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Whole model\n",
    "---\n",
    "##### Note that:\n",
    "- Data is feeded to the model by batches. For example, if we set batch size to 64 and our dataset consists of 28x28x1 images, we feed to the model tensor of size [64, 28, 28, 1] (in the below image batch size is ignored!);\n",
    "- Commonly, before dense layer feature maps are flattened by averaging in height and width dimension. Thus feature map of shape [64, 28, 28, 264] becomes [64, 264].\n",
    "<img src=\"images/cnn/whole_model.jpeg\" height=\"100\" width=\"1500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Collection and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Our Task\n",
    "\n",
    "<img src=\"images/cnn/Classification-vs-Classification-and-Localization.png\" height=\"600\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our dataset consists of 690 images of faces with masks and 686 images of faces without the mask. Data is somewhat artificial but still captures the real world. The data creation process has two steps:\n",
    "\n",
    "* Finding normal face images\n",
    "\n",
    "* Applying some computer vision tricks to add face masks to them\n",
    "\n",
    "\n",
    "[data source](https://github.com/prajnasb/observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The \"trick\" here is to use [Facial Landmarks](https://paperswithcode.com/task/facial-landmark-detection) to automatically detect facial structures, such as eyes, eyebrows, mouth, nose, and jawline. For building a dataset, we need images of faces without a mask.\n",
    "\n",
    "<img src=\"images/cnn/face.jpg\" height=\"500\" width=\"500\">\n",
    "\n",
    "[source](https://www.pyimagesearch.com/2020/05/04/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have to apply face detection to calculate the face bounding box.\n",
    "\n",
    "<img src=\"images/cnn/face_bounding_box.jpg\" height=\"500\" width=\"500\">\n",
    "\n",
    "\n",
    "[source](https://www.pyimagesearch.com/2020/05/04/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Having a face bounding box gives us the ability to apply facial landmarks and localize facial structure.\n",
    "\n",
    "<img src=\"images/cnn/facial_landmarks.png\" height=\"500\" width=\"500\">\n",
    "\n",
    "\n",
    "[source](https://www.pyimagesearch.com/2020/05/04/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the next step, the mask will be applied to the face.\n",
    "\n",
    "<img src=\"images/cnn/face_with_mask.jpg\" height=\"500\" width=\"500\">\n",
    "\n",
    "\n",
    "[source](https://www.pyimagesearch.com/2020/05/04/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is a one **downside** of this dataset. We cannot use images of faces without a mask, which was used to generate images with a face mask for model training. Doing otherwise will cause the model to be biased and will not generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MobileNets\n",
    "\n",
    "A class of efficient models for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses **depth-wise separable convolutions** to build light weight deep neural networks. Two main versions among others:\n",
    "\n",
    "\n",
    "* MobileNetV1\n",
    "\n",
    "\n",
    "* MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/cnn/mobilenets.png\" height=\"1000\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MobileNetV1\n",
    "\n",
    "---\n",
    "\n",
    "**Depthwise Convolution**, that applies a single convolution filter per input channel to perform lightweight filtering.\n",
    "\n",
    "**Pointwise Convolution**. This is a $(1\\times1)$ convolution responsible for building new features through computing linear combinations of the input channels.\n",
    "\n",
    "Combining these two types of convolution operation, we get **Depthwise Separable Convolution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MobileNetV2\n",
    "\n",
    "---\n",
    "\n",
    "In this network, there are two types of blocks: The first is the residual block with stride 1, and the second is with stride 2 for downsizing.\n",
    "\n",
    "There are 3 layers:\n",
    "\n",
    "* The first layer is $(1\\times1)$ convolution ReLU6\n",
    "\n",
    "\n",
    "* The second layer is depthwise convolution\n",
    "\n",
    "\n",
    "* The third layer is another $(1\\times1)$ convolution without any non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><strong>ImageNet Classification</strong></center>\n",
    "\n",
    "<img src=\"images/cnn/imagenet.png\" height=\"900\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As mentioned above, for face mask detection we used MobileNetV2 architecture.\n",
    "\n",
    "To improve model's generalization ability we used [data augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation), where random rotation, zoom, shift, shear, and flip of images are established. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Load the MobileNetV2 with pre-trained ImageNet weights.\n",
    "# Ensuring the head FC layer sets are left off\n",
    "\n",
    "baseModel = MobileNetV2(weights=\"imagenet\",\n",
    "                        include_top=False,\n",
    "                        input_tensor=Input(shape=(224, 224, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Construct the head of the model that will be placed on top of the base model\n",
    "\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "\n",
    "\n",
    "# Place the head FC model on top of the base model\n",
    "# (this will become the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizer function\n",
    "\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "\n",
    "# Train the head of the network\n",
    "\n",
    "H = model.fit(\n",
    "    aug.flow(X_train,\n",
    "             Y_train,\n",
    "             batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    validation_steps=len(X_test) // BATCH_SIZE,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification Report\n",
    "\n",
    "|              | precision | recall | f1-score | support |\n",
    "|--------------|-----------|--------|----------|---------|\n",
    "| with_mask    | 0.99      | 1.00   | 0.99     | 138     |\n",
    "| without_mask | 1.00      | 0.99   | 0.99     | 138     |\n",
    "| accuracy     |           |        | 0.99     | 276     |\n",
    "| macro avg    | 0.99      | 0.99   | 0.99     | 276     |\n",
    "| weighted avg | 0.99      | 0.99   | 0.99     | 276     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training Loss and Accuracy\n",
    "\n",
    "<img src=\"./images/cnn/plot.png\" height=\"900\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Face Mask Detection in Real-Time Video Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "[MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)\n",
    "\n",
    "[MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)\n",
    "\n",
    "[Depthwise separable convolutions for machine learning](https://eli.thegreenplace.net/2018/depthwise-separable-convolutions-for-machine-learning/)\n",
    "\n",
    "[MobileNetV2: The Next Generation of On-Device Computer Vision Networks](https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html)\n",
    "\n",
    "[Google’s MobileNets on the iPhone](https://machinethink.net/blog/googles-mobile-net-architecture-on-iphone/)\n",
    "\n",
    "[MobileNet version 2](https://machinethink.net/blog/mobilenet-v2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/cnn/questions.jpg\" height=\"600\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><strong style=\"font-size:40px\">Thank You</strong></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "---\n",
    "\n",
    "https://github.com/MaxinAI/school-of-ai\n",
    "\n",
    "https://www.pyimagesearch.com/2020/05/04/covid-19-face-mask-detector-with-opencv-keras-tensorflow-and-deep-learning/\n",
    "\n",
    "https://github.com/prajnasb/observations"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
